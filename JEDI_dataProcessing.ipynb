{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TzQEAkRJDNS"
   },
   "source": [
    "# VolPy pipeline for processing voltage imaging data \n",
    "The processing pipeline includes motion correction, memory mapping, segmentation, denoising and source extraction. The demo shows how to construct the params, MotionCorrect and VOLPY objects and call the relevant functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EW54aHS_HRnE",
    "outputId": "6fd13521-983f-4711-ac76-fc3a601075f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import imageio\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "        \n",
    "        get_ipython().run_line_magic('autoreload', '2')\n",
    "        #get_ipython().run_line_magic('matplotlib', 'qt')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.utils.utils import download_demo, download_model\n",
    "from caiman.source_extraction.volpy import utils\n",
    "from caiman.source_extraction.volpy.volparams import volparams\n",
    "from caiman.source_extraction.volpy.volpy import VOLPY\n",
    "from caiman.source_extraction.volpy.mrcnn import visualize, neurons\n",
    "import caiman.source_extraction.volpy.mrcnn.model as modellib\n",
    "from caiman.summary_images import local_correlations_movie_offline\n",
    "from caiman.summary_images import mean_image\n",
    "from caiman.paths import caiman_datadir\n",
    "\n",
    "logfile = None # Replace with a path if you want to log to a file\n",
    "logger = logging.getLogger('caiman')\n",
    "logger.setLevel(logging.ERROR)\n",
    "logfmt = logging.Formatter('%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s] [%(process)d] %(message)s')\n",
    "if logfile is not None:\n",
    "    handler = logging.FileHandler(logfile)\n",
    "else:\n",
    "    handler = logging.StreamHandler()\n",
    "handler.setFormatter(logfmt)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "# Setup some parameters for data and motion correction dataset parameters\n",
    "fr = 147                                        # sample rate of the movie\n",
    "fnames = []\n",
    "ROIs = None                                     # Region of interests\n",
    "index = None                                    # index of neurons\n",
    "weights = None                                  # reuse spatial weights by \n",
    "                                                # opts.change_params(params_dict={'weights':vpy.estimates['weights']})\n",
    "# Motion correction parameters\n",
    "pw_rigid = False                                # flag for pw-rigid motion correction\n",
    "gSig_filt = (3, 3)                              # size of filter, in general gSig (see below),\n",
    "                                                # change this one if algorithm does not work\n",
    "max_shifts = (5, 5)                             # maximum allowed rigid shift\n",
    "strides = (48, 48)                              # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24)                             # overlap between patches (size of patch strides+overlaps)\n",
    "max_deviation_rigid = 3                         # maximum deviation allowed for patch with respect to rigid shifts\n",
    "border_nan = 'copy'\n",
    "\n",
    "opts_dict = {\n",
    "    'fnames': fnames,\n",
    "    'fr': fr,\n",
    "    'index': index,\n",
    "    'ROIs': ROIs,\n",
    "    'weights': weights,\n",
    "    'pw_rigid': pw_rigid,\n",
    "    'max_shifts': max_shifts,\n",
    "    'gSig_filt': gSig_filt,\n",
    "    'strides': strides,\n",
    "    'overlaps': overlaps,\n",
    "    'max_deviation_rigid': max_deviation_rigid,\n",
    "    'border_nan': border_nan,\n",
    "    'use_cuda': False\n",
    "}\n",
    "\n",
    "opts = volparams(params_dict=opts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up File Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EW54aHS_HRnE",
    "outputId": "6fd13521-983f-4711-ac76-fc3a601075f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File path to movie file (will download if not present)\n",
    "# Create mean and correlation images\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "\n",
    "Subject = 'H1564'\n",
    "Date ='2025-03-31'\n",
    "Sequence = ''\n",
    "S2PDirectory = '/HwangLab/Suite2P/'+Date+'/'+ Subject+'/'+Subject + '-' + Date + '-2P' + Sequence+'/plane1'\n",
    "\n",
    "tif_file = S2PDirectory + '/data.tif'\n",
    "with tifffile.TiffFile(tif_file) as tiff:\n",
    "    dims = tiff.series[0].shape\n",
    "\n",
    "print(f\"Tiff file dimenions: {dims}\")\n",
    "\n",
    "fnames = [tif_file]\n",
    "parent, stem = os.path.split(fnames[0])\n",
    "save_base_name = parent + '/' + 'data'\n",
    "fname_tot = cm.paths.memmap_frames_filename(save_base_name, dims[1:], dims[0], order='F')\n",
    "fname_new = fname_tot\n",
    "mmap_file= fname_tot\n",
    "parent, stem = os.path.split(fname_tot)\n",
    "path_ROIs = parent + '/' + stem[:-5] + '_mrcnn_ROIs.hdf5'\n",
    "print(f'ROI file: {path_ROIs}')\n",
    "print(f'mmap file: {fname_tot}')\n",
    "\n",
    "\n",
    "opts_dict = {\n",
    "    'fnames': fnames,\n",
    "    'fr': fr,\n",
    "    'index': index,\n",
    "    'ROIs': ROIs,\n",
    "    'weights': weights,\n",
    "    'pw_rigid': pw_rigid,\n",
    "    'max_shifts': max_shifts,\n",
    "    'gSig_filt': gSig_filt,\n",
    "    'strides': strides,\n",
    "    'overlaps': overlaps,\n",
    "    'max_deviation_rigid': max_deviation_rigid,\n",
    "    'border_nan': border_nan,\n",
    "    'use_cuda': False\n",
    "}\n",
    "\n",
    "opts.change_params(params_dict=opts_dict); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SKIP THIS IF MEMMAP ALREADY EXISTS\n",
    "if not os.path.exists(fname_tot):\n",
    "#if 1:\n",
    "    m_reg = cm.load(tif_file).astype(np.float16)\n",
    "    print(f'Done reading, dimension: {m_reg.shape}')\n",
    "#    parent, stem = os.path.split(fnames[0])\n",
    "#    save_base_name = parent +'/'+stem[:-4]\n",
    "\n",
    "    order = 'F'\n",
    "    dims = m_reg.shape\n",
    "    print(dims)\n",
    "    fname_tot = cm.paths.memmap_frames_filename(save_base_name, dims[1:], dims[0], order)\n",
    "    big_mov = np.memmap(fname_tot, mode='w+', dtype=np.float16,\n",
    "                shape=cm.mmapping.prepare_shape((np.prod(dims[1:]), dims[0])), order=order)\n",
    "    big_mov[:] = np.reshape(m_reg.transpose(1, 2, 0), (np.prod(dims[1:]), dims[0]), order='F')\n",
    "    big_mov.flush()\n",
    "    del big_mov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Multiprocessing before ROI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TLSPvv5Nt-u-"
   },
   "outputs": [],
   "source": [
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='multiprocessing', n_processes=None, single_thread=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mean_image(mmap_file, window = 1000, dview=dview)\n",
    "\n",
    "img = (img-np.nanmean(img))/np.nanstd(img)\n",
    "\n",
    "gaussian_blur = False        # Use gaussian blur when there is too much noise in the video\n",
    "Cn = local_correlations_movie_offline(mmap_file, fr=fr, window=fr*4, \n",
    "                                      stride=fr*4, winSize_baseline=fr, \n",
    "                                      remove_baseline=True, gaussian_blur=gaussian_blur,\n",
    "                                      dview=dview).max(axis=0)\n",
    "img_corr = (Cn-np.mean(Cn))/np.std(Cn)\n",
    "summary_images = np.stack([img, img, img_corr], axis=0).astype(np.float16)\n",
    "\n",
    "np.save(S2PDirectory+'/summary_images.npy', summary_images)\n",
    "# Save summary images which could be further used in the VolPy GUI\n",
    "cm.movie(summary_images).save(tif_file[:-5] + '_summary_images.tif')\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(summary_images[0]); axs[1].imshow(summary_images[2])\n",
    "axs[0].set_title('mean image'); axs[1].set_title('corr image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use ROI detected by Suite2P\n",
    "print(S2PDirectory)\n",
    "stat = np.load(S2PDirectory+'/stat.npy', allow_pickle=True)\n",
    "\n",
    "summary_images = np.load(S2PDirectory +'/summary_images.npy')\n",
    "print(summary_images[0].shape)\n",
    "ROIs = []\n",
    "for cell in range(len(stat)):\n",
    "    roi = np.full_like(summary_images[0], False)\n",
    "    for y, x in zip(stat[cell]['ypix'], stat[cell]['xpix']):\n",
    "        roi[y][x] = True\n",
    "    ROIs.append(roi)\n",
    "ROIs = np.array(ROIs)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(summary_images[0]); \n",
    "axs[1].imshow(ROIs.sum(0))\n",
    "axs[0].set_title('mean image'); axs[1].set_title('masks')\n",
    "print(f\"Number of ROIS:{len(ROIs)}\")\n",
    "\n",
    "cm.movie(ROIs).save(path_ROIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up Multiprocessing Before Spike Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6z92CC4631AA"
   },
   "outputs": [],
   "source": [
    "# Restart cluster to clean up memory\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "#cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='multiprocessing', n_processes=None, single_thread=False, maxtasksperchild=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace Denoising and Spike Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wCNTy-LZHRn6"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Parameters for trace denoising and spike extraction\n",
    "with h5py.File(path_ROIs, 'r') as fl:\n",
    "    ROIs = fl['mov'][()]  # load ROIs\n",
    "\n",
    "ROIs = ROIs                                   # region of interests\n",
    "index = list(range(len(ROIs)))                # index of neurons\n",
    "weights = None                                # if None, use ROIs for initialization; to reuse weights check reuse weights block \n",
    "\n",
    "template_size = 0.02                          # half size of the window length for spike templates, default is 20 ms \n",
    "context_size = 70                             # number of pixels surrounding the ROI to censor from the background PCA for 2X imaging sessions: use 35 for 1X imaging sessions\n",
    "visualize_ROI = False                         # whether to visualize the region of interest inside the context region\n",
    "flip_signal = True                            # Important!! Flip signal or not, True for Voltron indicator, False for others\n",
    "hp_freq_pb = 1 / 3                            # parameter for high-pass filter to remove photobleaching\n",
    "clip = 100                                    # maximum number of spikes to form spike template\n",
    "threshold_method = 'simple'                   # adaptive_threshold or simple \n",
    "min_spikes= 10                                # minimal spikes to be found\n",
    "pnorm = 0.5                                   # a variable deciding the amount of spikes chosen for adaptive threshold method\n",
    "threshold = 2                                 # threshold for finding spikes only used in simple threshold method, Increase the threshold to find less spikes\n",
    "do_plot = False                               # plot detail of spikes, template for the last iteration\n",
    "ridge_bg= 0.01                                # ridge regression regularizer strength for background removement, larger value specifies stronger regularization \n",
    "sub_freq = 20                                 # frequency for subthreshold extraction\n",
    "weight_update = 'ridge'                       # ridge or NMF for weight update\n",
    "n_iter = 2                                    # number of iterations alternating between estimating spike times and spatial filters\n",
    "\n",
    "for i in range(0,len(ROIs),1):\n",
    "    index = [i]\n",
    "\n",
    "    opts_dict={'fnames':fname_new,\n",
    "                'ROIs': ROIs,\n",
    "                'index': index,\n",
    "                'weights': weights,\n",
    "                'template_size': template_size, \n",
    "                'context_size': context_size,\n",
    "                'visualize_ROI': visualize_ROI, \n",
    "                'flip_signal': flip_signal,\n",
    "                'hp_freq_pb': hp_freq_pb,\n",
    "                'clip': clip,\n",
    "                'threshold_method': threshold_method,\n",
    "                'min_spikes':min_spikes,\n",
    "                'pnorm': pnorm, \n",
    "                'threshold': threshold,\n",
    "                'do_plot':do_plot,\n",
    "                'ridge_bg':ridge_bg,\n",
    "                'sub_freq': sub_freq,\n",
    "                'weight_update': weight_update,\n",
    "                'n_iter': n_iter}\n",
    "\n",
    "    opts.change_params(params_dict=opts_dict);    \n",
    "\n",
    "    n_processes = 1\n",
    "    vpy = VOLPY(n_processes=n_processes, dview=dview, params=opts)\n",
    "    print(f'Setting_ROI{i}_{datetime.datetime.now()}')\n",
    "    vpy.fit(n_processes=n_processes, dview=dview)\n",
    "    vpy.estimates['ROIs'] = ROIs\n",
    "    save_name = f'volpy_{os.path.split(fname_tot)[1][:-5]}_{threshold_method}_{i}'\n",
    "    np.save(os.path.join(S2PDirectory, save_name), vpy.estimates)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "L9Otq9-hJMMj"
   },
   "outputs": [],
   "source": [
    "# Visualize spatial footprints and traces\n",
    "#idx = np.where(vpy.estimates['locality'] > 0)[0]\n",
    "#utils.view_components(vpy.estimates, img_corr, idx)\n",
    "# Visualize spatial footprints and traces\n",
    "i=0\n",
    "#volpy_name = f'volpy_{os.path.split(fname_tot)[1][:-5]}_adaptive_threshold_{i}'\n",
    "volpy_name = f'volpy_{os.path.split(fname_tot)[1][:-5]}_{threshold_method}_{i}'\n",
    "\n",
    "vpy = np.load(S2PDirectory + '/' + volpy_name + '.npy', allow_pickle=True).item()\n",
    "idx = np.where(vpy['locality'] > 0)[0]\n",
    "print(idx)\n",
    "utils.view_components(vpy, img_corr, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike Gaussian Convolution Per ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "L9Otq9-hJMMj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "def event_times_to_rate(event_times, time_window, kernel_width, time_step=1):\n",
    "    \"\"\"\n",
    "    Fast conversion of event times to event rate using a Gaussian kernel.\n",
    "\n",
    "    Parameters:\n",
    "        event_times (array-like): Sequence of event times.\n",
    "        time_window (tuple): Start and end time of the observation window.\n",
    "        kernel_width (float): Standard deviation of the Gaussian kernel.\n",
    "        time_step (float): Time resolution of the rate function.\n",
    "\n",
    "    Returns:\n",
    "        times (np.ndarray): Array of time points.\n",
    "        rates (np.ndarray): Event rate at each time point.\n",
    "    \"\"\"\n",
    "    # Generate time points\n",
    "    times = np.arange(time_window[0], time_window[1], time_step)\n",
    "    \n",
    "    # Create a Gaussian kernel\n",
    "    kernel_half_width = 4 * kernel_width  # Use 4 standard deviations for truncation\n",
    "    kernel_times = np.arange(-kernel_half_width, kernel_half_width, time_step)\n",
    "    kernel = np.exp(-0.5 * (kernel_times / kernel_width)**2)\n",
    "    kernel /= kernel.sum()  # Normalize kernel to preserve total event count\n",
    "    \n",
    "    # Create a histogram of event times\n",
    "    event_histogram, _ = np.histogram(event_times, bins=len(times), range=time_window)\n",
    "    \n",
    "    # Convolve the histogram with the Gaussian kernel\n",
    "    rates = fftconvolve(event_histogram, kernel, mode='same') / time_step\n",
    "    \n",
    "    return times, rates\n",
    "\n",
    "with h5py.File(path_ROIs, 'r') as fl:\n",
    "    ROIs = fl['mov'][()]  # load ROIs\n",
    "\n",
    "FiringRate = []\n",
    "Spikes_time = []\n",
    "snr = []\n",
    "low = []\n",
    "cell = []\n",
    "for i in range(len(ROIs)):\n",
    "    volpy_name = f\"volpy_{os.path.split(fname_tot)[1][:-5]}_{threshold_method}_{i}\"\n",
    "\n",
    "    vpy = np.load(S2PDirectory + '/' + volpy_name + '.npy', allow_pickle=True).item()\n",
    "    event_times = vpy['spikes'][0]\n",
    "    time_window = (0, len(vpy['t'][0]))\n",
    "    kernel_width = 0.01*fr\n",
    "    \n",
    "    #print(vpy['snr'][0], vpy['low_spikes'][0])\n",
    "\n",
    "    times, rates = event_times_to_rate(event_times, time_window, kernel_width)\n",
    "    FiringRate.append(rates)\n",
    "    Spikes_time.append(event_times)\n",
    "    snr.append(vpy['snr'][0])\n",
    "    low.append(vpy['low_spikes'][0])\n",
    "    cell.append(vpy['num_spikes'][0][-1]/len(vpy['t'][0])*fr>0.05)\n",
    "\n",
    "FiringRates = np.array(FiringRate)*fr\n",
    "SpikesTimes = np.array(Spikes_time, dtype=object)\n",
    "SNRs = np.array(snr)\n",
    "Sparse = np.array(low)\n",
    "# cells = np.array(cell)\n",
    "# iscell = np.vstack((cells*1,SNRs)).T\n",
    "\n",
    "np.save(S2PDirectory + '/rates.npy', FiringRates )\n",
    "np.save(S2PDirectory + '/SpikesTimes.npy', SpikesTimes)\n",
    "np.save(S2PDirectory + '/SNR.npy', SNRs)\n",
    "np.save(S2PDirectory + '/Sparse.npy', Sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jv9zCNpbHRoO"
   },
   "outputs": [],
   "source": [
    "# Stop cluster and clean up log files\n",
    "cm.stop_server(dview=dview)\n",
    "log_files = glob.glob('*_LOG_*')\n",
    "for log_file in log_files:\n",
    "    os.remove(log_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (caiman)",
   "language": "python",
   "name": "caiman"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
